{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctPspYPHd8Me"
      },
      "source": [
        "# **0. Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_x_DIyBAddxP",
        "outputId": "2112a7f3-ee02-4391-a772-6e01a30844a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (25.1.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "#@title 0.1 Install Dependencies\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install torchmetrics\n",
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYgvFOxxdpVy"
      },
      "outputs": [],
      "source": [
        "#@title 0.2 Import Dependencies\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, SubsetRandomSampler\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics.functional import accuracy, precision, recall, auroc\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "from torchmetrics import F1Score\n",
        "from torchmetrics.classification import Accuracy, Precision, Recall\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import torch\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "\n",
        "import requests\n",
        "import io\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZxeLBv2hFS-"
      },
      "outputs": [],
      "source": [
        "#@title 0.3 Set Device\n",
        "\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YQ0fTV2eAil"
      },
      "source": [
        "# **1. Reading File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY2vTlvwdw0r"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "\n",
        "urls = {\n",
        "    \"train\": \"https://drive.google.com/uc?id=1he-dSeLCYl5lLIEbndfXOu4195jLgxrC\",\n",
        "    \"val\": \"https://drive.google.com/uc?id=18qITfL87wdNX0EAq-OwLFliRv6KmCEVA\",\n",
        "    \"test\": \"https://drive.google.com/uc?id=1aVxq8xkR4fFUpb4vw9jyyFlmC0woASNd\"\n",
        "}\n",
        "\n",
        "# Download each file\n",
        "for name, url in urls.items():\n",
        "    output_path = f\"/content/{name}.jsonl\"  # Save each file with its respective name\n",
        "    gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "print(\"All files downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csBpnGQ0Jhn6"
      },
      "source": [
        "\n",
        "We implement a function that reads data from a `.jsonl` file and creates a DataFrame from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lEeXyYsgFYJ"
      },
      "outputs": [],
      "source": [
        "def load_jsonl(file_path):\n",
        "    data = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTSFCf4SJhn7"
      },
      "source": [
        "Now, using the previous function, we create DataFrames for:\n",
        "\n",
        "1.   Train Set\n",
        "2.   Validation Set\n",
        "3.   Test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KU6WRoZgHl3"
      },
      "outputs": [],
      "source": [
        "train_df = load_jsonl(\"train.jsonl\")\n",
        "val_df = load_jsonl(\"val.jsonl\")\n",
        "test_df = load_jsonl(\"test.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWRb2e_pJhn7"
      },
      "source": [
        "We display the DataFrames, and as we can observe, they contain the following columns:\n",
        "\n",
        "\n",
        "1.  `annotator_labels` – A list of labels representing the annotators' votes (neutral, entailment, contradiction). The length of this list varies from 1 to 5, depending on the number of votes cast.\n",
        "\n",
        "2.  `captionID` - The ID of the first sentence (premise).\n",
        "\n",
        "3.  `gold_label` - The label determined by the majority vote of the annotators (see `annotator_labels`)\n",
        "\n",
        "4. `pairID` – The ID of the sentence pair (premise and hypothesis).\n",
        "\n",
        "5. `sentence1` – The first sentence (premise).\n",
        "\n",
        "6. `sentence1_binary_parse` - The first sentence formatted for tree-structured neural networks with no unary nodes and no labels.\n",
        "\n",
        "7. `sentence1_parse` - The first sentence formatted for tree-structured neural networks with no unary nodes and no labels.\n",
        "\n",
        "8. `sentence2` - The second sentence (hypothesis).\n",
        "\n",
        "9. `sentence1_binary_parse` - The second sentence formatted for tree-structured neural networks with no unary nodes and no labels.\n",
        "\n",
        "10. `sentence2_parse` - The second sentence parsed using the Stanford Parser in Penn Treebank format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZlUGHa_gPOZ"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wNFH6XNbyop"
      },
      "outputs": [],
      "source": [
        "val_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2nC346zb2OR"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Ah9IgnJhn8"
      },
      "source": [
        "Now, we create three DataFrames containing only the necessary labels for our models. We will filter them to retain only valid labels, specifically those in `['entailment', 'neutral', 'contradiction']`.\n",
        "The resulting DataFrames will include the following columns:\n",
        "\n",
        "1.   `sentence1` – The first sentence (premise)\n",
        "\n",
        "2.   `sentence2` – The second sentence (hypothesis).\n",
        "\n",
        "3.   `gold_label` – The majority vote label from the annotators.\n",
        "\n",
        "This ensures that we work only with relevant and properly labeled data for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwdRGKrmE3ff"
      },
      "outputs": [],
      "source": [
        "train_df = train_df[['sentence1', 'sentence2', 'gold_label']]\n",
        "val_df = val_df[['sentence1', 'sentence2', 'gold_label']]\n",
        "test_df = test_df[['sentence1', 'sentence2', 'gold_label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f4ZLLh4intm"
      },
      "outputs": [],
      "source": [
        "valid_labels = ['entailment', 'neutral', 'contradiction']\n",
        "\n",
        "train_df_invalid = train_df[~train_df['gold_label'].isin(valid_labels)]\n",
        "val_df_invalid = val_df[~val_df['gold_label'].isin(valid_labels)]\n",
        "test_df_invalid = test_df[~test_df['gold_label'].isin(valid_labels)]\n",
        "\n",
        "print(f\"Invalid Train: {len(train_df_invalid)}, Invalid Validation: {len(val_df_invalid)}, Invalid Test: {len(test_df_invalid)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu-vA0wrj1-E"
      },
      "outputs": [],
      "source": [
        "train_df_invalid.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsrBQg2GgqL7"
      },
      "outputs": [],
      "source": [
        "train_df = train_df[train_df['gold_label'].isin(valid_labels)]\n",
        "val_df = val_df[val_df['gold_label'].isin(valid_labels)]\n",
        "test_df = test_df[test_df['gold_label'].isin(valid_labels)]\n",
        "\n",
        "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AT_X6aZjgy5"
      },
      "source": [
        "Now, we assign a numerical value to each label in gold_label using the following mapping:\n",
        "\n",
        "`{'entailment': 0, 'contradiction': 1, 'neutral': 2}`\n",
        "\n",
        "We then apply this transformation to the training set and display the updated DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXfvcn-mgsfW"
      },
      "outputs": [],
      "source": [
        "label_mapping = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
        "train_df['gold_label'] = train_df['gold_label'].map(label_mapping)\n",
        "val_df['gold_label'] = val_df['gold_label'].map(label_mapping)\n",
        "test_df['gold_label'] = test_df['gold_label'].map(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwRc-z8XjD6V"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrOAyTVQeocU"
      },
      "source": [
        "## 1.1 Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIbw-Hu7kPGs"
      },
      "source": [
        "We visualize the length distribution of the two sentences using the graphs below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8wvTWcIVYWz"
      },
      "outputs": [],
      "source": [
        "train_df['sentence1_length'] = train_df['sentence1'].apply(lambda x: len(x.split()))\n",
        "train_df['sentence2_length'] = train_df['sentence2'].apply(lambda x: len(x.split()))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(train_df['sentence1_length'], bins=50, kde=True)\n",
        "plt.title('Histogram of Sentence 1 Length')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(train_df['sentence2_length'], bins=50, kde=True)\n",
        "plt.title('Histogram of Sentence 2 Length')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZlxYoBYnsd9"
      },
      "outputs": [],
      "source": [
        "all_lengths = pd.concat([train_df['sentence1_length'], train_df['sentence2_length']], ignore_index=True)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.histplot(all_lengths, bins=50, kde=True)\n",
        "plt.title('Histogram of All Sentence Lengths')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp2Hh193lOXb"
      },
      "outputs": [],
      "source": [
        "train_df[['sentence1_length','sentence2_length']].describe().T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K1yIYXvku9l"
      },
      "source": [
        "We can observe that the two distributions are very similar and right-skewed. Above, we display the main statistical characteristics.\n",
        "\n",
        "Below, we show the shortest and longest sentences for `sentence1` and `sentence2`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKt1PiSMmRhF"
      },
      "outputs": [],
      "source": [
        "shortest_sentence1 = train_df['sentence1_length'].min()\n",
        "shortest_sentence1_row = train_df[train_df['sentence1_length'] == shortest_sentence1]\n",
        "print(f\"Shortest sentence in sentence1 (length: {shortest_sentence1}):\")\n",
        "print(shortest_sentence1_row[['sentence1', 'sentence1_length']].iloc[0]['sentence1'])\n",
        "\n",
        "\n",
        "shortest_sentence2 = train_df['sentence2_length'].min()\n",
        "shortest_sentence2_row = train_df[train_df['sentence2_length'] == shortest_sentence2]\n",
        "print(f\"\\nShortest sentence in sentence2 (length: {shortest_sentence2}):\")\n",
        "print(shortest_sentence2_row[['sentence2', 'sentence2_length']].iloc[0]['sentence2'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWwvBGEBnz6Z"
      },
      "outputs": [],
      "source": [
        "longest_sentence1 = train_df['sentence1_length'].max()\n",
        "longest_sentence1_row = train_df[train_df['sentence1_length'] == longest_sentence1]\n",
        "print(f\"Longest sentence in sentence1 (length: {longest_sentence1}):\")\n",
        "print(longest_sentence1_row[['sentence1', 'sentence1_length']].iloc[0]['sentence1'])\n",
        "\n",
        "longest_sentence2 = train_df['sentence2_length'].max()\n",
        "longest_sentence2_row = train_df[train_df['sentence2_length'] == longest_sentence2]\n",
        "print(f\"\\nLongest sentence in sentence2 (length: {longest_sentence2}):\")\n",
        "print(longest_sentence2_row[['sentence2', 'sentence2_length']].iloc[0]['sentence2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GfKKXAfoNXk"
      },
      "source": [
        "We visualize the length of the two sentences up to the 95th percentile to observe the maximum sentence length that generally occurs.\n",
        "\n",
        "From the red line, we can see that at the 95th percentile, the sentence lengths are **23 for the premise** and **18 for the hypothesis**. Moreover, when considering all sentences, this length becomes **20**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs-bvbQNWUI0"
      },
      "outputs": [],
      "source": [
        "percentile_95_sentence1 = train_df['sentence1_length'].quantile(0.95)\n",
        "percentile_95_sentence2 = train_df['sentence2_length'].quantile(0.95)\n",
        "\n",
        "print(f\"95th percentile of sentence1 length: {percentile_95_sentence1}\")\n",
        "print(f\"95th percentile of sentence2 length: {percentile_95_sentence2}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(train_df['sentence1_length'], bins=50, kde=True)\n",
        "plt.axvline(percentile_95_sentence1, color='r', linestyle='--', label='95th Percentile')\n",
        "plt.title('Histogram of Sentence 1 Length with 95th Percentile')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(train_df['sentence2_length'], bins=50, kde=True)\n",
        "plt.axvline(percentile_95_sentence2, color='r', linestyle='--', label='95th Percentile')\n",
        "plt.title('Histogram of Sentence 2 Length with 95th Percentile')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROvUYTPCpdql"
      },
      "outputs": [],
      "source": [
        "all_lengths = pd.concat([train_df['sentence1_length'], train_df['sentence2_length']], ignore_index=True)\n",
        "\n",
        "percentile_95_all = all_lengths.quantile(0.95)\n",
        "\n",
        "print(f\"95th percentile of all sentence lengths: {percentile_95_all}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(all_lengths, bins=50, kde=True)\n",
        "plt.axvline(percentile_95_all, color='r', linestyle='--', label='95th Percentile')\n",
        "\n",
        "plt.title('Histogram of All Sentence Lengths with 95th Percentile')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNd1J5Iwpw0_"
      },
      "source": [
        "As we can observe, the pie chart of the label distribution is perfectly balanced. This is due to the fact that each premise is repeated **three times**, with a different hypothesis assigned to each instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5z7KC67YKAe"
      },
      "outputs": [],
      "source": [
        "label_counts = train_df['gold_label'].value_counts()\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Distribution of Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW8P3tqwYOt0"
      },
      "outputs": [],
      "source": [
        "train_df.head(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8QX_VfGgzeJ"
      },
      "source": [
        "# **2. Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C-Nect7sEiz"
      },
      "source": [
        "The first implemented model, explained in more detail in the report, is based on six key steps:\n",
        "\n",
        "1.   **Feature Extraction**\n",
        "2.   **DataLoader**\n",
        "3.   **Model Definition**\n",
        "4.   **Hyperparameter Tuning**\n",
        "5.   **Training**\n",
        "6.   **Evaluation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTVF86Tagi35"
      },
      "source": [
        "## 2.1 Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWGW75LNtEpe"
      },
      "source": [
        "\n",
        "In this step, we transform each sentence (premise and hypothesis) into a vector using the **Hashing Vectorizer function** with **10,007 features** to standardize the input dimension of the model.\n",
        "\n",
        "Due to the large dataset size and the high number of features, the result will be a **sparse matrix**, as storing it as a dense matrix would require excessive RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UYKr8VogvjR"
      },
      "outputs": [],
      "source": [
        "vectorizer = HashingVectorizer(n_features=10007, alternate_sign=False, norm=None)\n",
        "\n",
        "X_train_premise_hashed = vectorizer.fit_transform(train_df['sentence1'])\n",
        "X_train_hypothesis_hashed = vectorizer.fit_transform(train_df['sentence2'])\n",
        "X_val_premise_hashed = vectorizer.transform(val_df['sentence1'])\n",
        "X_val_hypothesis_hashed = vectorizer.transform(val_df['sentence2'])\n",
        "X_test_premise_hashed = vectorizer.transform(test_df['sentence1'])\n",
        "X_test_hypothesis_hashed = vectorizer.transform(test_df['sentence2'])\n",
        "\n",
        "y_train = train_df['gold_label'].values\n",
        "y_val = val_df['gold_label'].values\n",
        "y_test = test_df['gold_label'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfQudzomg33X"
      },
      "source": [
        "## 2.2 DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrDsfM0IuHcn"
      },
      "source": [
        "We implement our dataset using the `class SNLIDataset(Dataset)`, which includes:\n",
        "\n",
        "1. **Input**: Two vectors, `X1` and `X2`, corresponding to the two sentences (premise and hypothesis).\n",
        "2. **Output**: The target variable `y`, corresponding to `gold_label`.\n",
        "\n",
        "Additionally, we convert the sparse matrix corresponding to each batch into a dense array for the model. By converting only the vectors within each batch, the matrix can be transformed into a dense array efficiently.\n",
        "\n",
        "Next, we create three datasets corresponding to the **training, validation, and test sets** using the class defined above. Finally, we implement the respective data loaders using `Dataloader` with a **batch size of 512** for training set and **64** for validation and test set.  \n",
        "\n",
        "Moreover, we shuffle (**shuffle=True**) only the training set to prevent the model from learning any specific order in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWuqTd7ng60e"
      },
      "outputs": [],
      "source": [
        "class SNLIDataset(Dataset):\n",
        "    def __init__(self, X1, X2, y):\n",
        "        self.X1 = X1  # Manteniamo sparse\n",
        "        self.X2 = X2\n",
        "        self.y = torch.tensor(y, dtype=torch.long)  # Convertiamo solo le etichette\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convertiamo solo il batch corrente in array denso\n",
        "        premise_dense = torch.tensor(self.X1[idx].toarray(), dtype=torch.float32).squeeze(0)\n",
        "        hypothesis_dense = torch.tensor(self.X2[idx].toarray(), dtype=torch.float32).squeeze(0)\n",
        "\n",
        "        return premise_dense, hypothesis_dense, self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96F8FW6rg_a1"
      },
      "outputs": [],
      "source": [
        "train_dataset = SNLIDataset(X_train_premise_hashed, X_train_hypothesis_hashed, y_train)\n",
        "val_dataset = SNLIDataset(X_val_premise_hashed, X_val_hypothesis_hashed, y_val)\n",
        "test_dataset = SNLIDataset(X_test_premise_hashed, X_test_hypothesis_hashed, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep1YW8_9xtw0"
      },
      "source": [
        "Finally, we verify the dimensions of the two inputs (premise and hypothesis) in the training set and the corresponding output dimension.  \n",
        "\n",
        "As we can observe, the dimensions match:  \n",
        "- **Batch size** = 512  \n",
        "- **Number of features** = 10,007 (which corresponds to the length of each vector representing a sentence).  \n",
        "\n",
        "This confirms that the input structure is correctly formatted for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKgM8pFIhC4A"
      },
      "outputs": [],
      "source": [
        "premise, hypothesis, labels = next(iter(train_loader))\n",
        "print(f\"Premise shape: {premise.shape}, Hypothesis shape: {hypothesis.shape}, Labels shape: {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pptmV3rFhWJN"
      },
      "source": [
        "## 2.3 Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxLgwvquhXRO"
      },
      "outputs": [],
      "source": [
        "class SNLIMLP(pl.LightningModule):\n",
        "    def __init__(self, input_dim = 10007, dropout = 0.1, lr = 1e-4):\n",
        "        super(SNLIMLP, self).__init__()\n",
        "        self.lr = lr\n",
        "\n",
        "        self.training_losses = []\n",
        "        self.validation_accuracies = []\n",
        "        self.validation_losses = []\n",
        "\n",
        "        self.first_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.combined_net = nn.Sequential(\n",
        "            nn.Linear(512 * 2, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 3),\n",
        "        )\n",
        "\n",
        "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
        "        self.f1 = F1Score(task=\"multiclass\", num_classes=3, average=\"macro\")\n",
        "\n",
        "    def forward(self, premise, hypothesis):\n",
        "        premise_feat = self.first_net(premise)\n",
        "        hypothesis_feat = self.first_net(hypothesis)\n",
        "        combined_feat = torch.cat((premise_feat, hypothesis_feat), dim=1)\n",
        "        return self.combined_net(combined_feat)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        premise, hypothesis, labels = batch\n",
        "        logits = self(premise, hypothesis)\n",
        "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.accuracy(preds, labels)\n",
        "        f1 = self.f1(preds, labels)\n",
        "\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        premise, hypothesis, labels = batch\n",
        "        logits = self(premise, hypothesis)\n",
        "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = self.accuracy(preds, labels)\n",
        "        f1 = self.f1(preds, labels)\n",
        "\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        try:\n",
        "            if self.trainer.logged_metrics[\"train_loss\"].cpu().item() not in self.training_losses:\n",
        "                self.training_losses.append(self.trainer.logged_metrics[\"train_loss\"].cpu().item())\n",
        "        except:\n",
        "            pass\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        premise, hypothesis, labels = batch\n",
        "        logits = self(premise, hypothesis)\n",
        "        acc = self.accuracy(logits, labels)\n",
        "        self.log(\"test_acc\", acc, prog_bar=True)\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        train_loss = self.trainer.callback_metrics[\"train_loss\"].item()\n",
        "        print(f\"Epoch {self.current_epoch} - Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        avg_val_loss = self.trainer.logged_metrics[\"val_loss\"].cpu().item()\n",
        "\n",
        "        avg_val_acc = self.trainer.logged_metrics[\"val_acc\"].cpu().item()\n",
        "\n",
        "        try:\n",
        "            if self.trainer.logged_metrics[\"val_acc\"].cpu().item() not in self.validation_accuracies:\n",
        "                self.validation_accuracies.append(self.trainer.logged_metrics[\"val_acc\"].cpu().item())\n",
        "            if self.trainer.logged_metrics[\"val_loss\"].cpu().item() not in self.validation_losses:\n",
        "                self.validation_losses.append(self.trainer.logged_metrics[\"val_loss\"].cpu().item())\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "        print(f\"[Validation] Epoch {self.current_epoch+1} - Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_acc:.4f}\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), self.lr, weight_decay=1e-3)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=2, factor=0.5)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nPkXNXaQY_8K"
      },
      "outputs": [],
      "source": [
        "#@title Model Visualization\n",
        "from graphviz import Digraph\n",
        "from IPython.display import display\n",
        "\n",
        "def draw_snli_classifier():\n",
        "    dot = Digraph(format=\"png\")\n",
        "    dot.attr(rankdir='LR')  # Disposizione orizzontale\n",
        "\n",
        "\n",
        "    dot.node(\"P\", \"Premise Input\\n(input_dim = 10007)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightblue\")\n",
        "    dot.node(\"H\", \"Hypothesis Input\\n(input_dim = 10007)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightblue\")\n",
        "\n",
        "    dot.node(\"F1_P\", \"Linear(10007->1024)\\nBatchNorm\\nLeakyReLU\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightgreen\")\n",
        "    dot.node(\"F2_P\", \"Linear(1024->512)\\nBatchNorm\\nLeakyReLU\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightgreen\")\n",
        "\n",
        "    dot.node(\"F1_H\", \"Linear(10007->1024)\\nBatchNorm\\nLeakyReLU\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightgreen\")\n",
        "    dot.node(\"F2_H\", \"Linear(1024->512)\\nBatchNorm\\nLeakyReLU\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightgreen\")\n",
        "\n",
        "    dot.node(\"Concat\", \"Linear(512 * 2->256)\\nBatchNorm\\nLeakyReLU\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"orange\")\n",
        "\n",
        "\n",
        "    dot.node(\"C1\", \"Linear(256->128)\\nBatchNorm\\nLeakyReLU\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightyellow\")\n",
        "    dot.node(\"C2\", \"Linear(128->3)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightyellow\")\n",
        "\n",
        "    dot.node(\"Class1\", \"Entailment (class 0) \\n(Softmax)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightcoral\")\n",
        "    dot.node(\"Class2\", \"Contradiction (class 1)\\n(Softmax)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightcoral\")\n",
        "    dot.node(\"Class3\", \"Neutral (class 2)\\n(Softmax)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightcoral\")\n",
        "\n",
        "    dot.edge(\"P\", \"F1_P\")\n",
        "    dot.edge(\"H\", \"F1_H\")\n",
        "    dot.edge(\"F1_P\", \"F2_P\")\n",
        "    dot.edge(\"F1_H\", \"F2_H\")\n",
        "    dot.edge(\"F2_P\", \"Concat\")\n",
        "    dot.edge(\"F2_H\", \"Concat\")\n",
        "    dot.edge(\"Concat\", \"C1\")\n",
        "    dot.edge(\"C1\", \"C2\")\n",
        "    dot.edge(\"C2\", \"Class1\")\n",
        "    dot.edge(\"C2\", \"Class2\")\n",
        "    dot.edge(\"C2\", \"Class3\")\n",
        "\n",
        "    display(dot)\n",
        "\n",
        "draw_snli_classifier()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKWoT3KmaOVJ"
      },
      "source": [
        "## 2.4 Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJLADAZ2eJzo"
      },
      "outputs": [],
      "source": [
        "def load_model_from_github(model_class, url, hidden_dim = 200, is_MLP = True, device = device):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    if is_MLP:\n",
        "        model = model_class()\n",
        "    else:\n",
        "        model = model_class(hidden_dim)\n",
        "    model.load_state_dict(torch.load(BytesIO(response.content), map_location=torch.device(device), weights_only=False))\n",
        "    model.to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCvUa4aMhkNC"
      },
      "source": [
        "If you want to **retrain the models**, set `load_models=False`.  \n",
        "\n",
        "Otherwise, if `load_models=True`, the pre-trained models will be loaded using the `load_model_from_github` function defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGSgKoekVGcE"
      },
      "outputs": [],
      "source": [
        "load_models = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah7w98_dT5_9"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.0001],\n",
        "    'dropout': [0.1, 0.3]\n",
        "}\n",
        "\n",
        "# Track best results\n",
        "best_val_acc = 0\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "if not load_models:\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    for lr in param_grid['learning_rate']:\n",
        "        for do in param_grid['dropout']:\n",
        "                print(f\"\\nlearning_rate={lr}, dropout={do}\")\n",
        "\n",
        "                model = SNLIMLP(\n",
        "                    input_dim=10007,\n",
        "                    dropout=do,\n",
        "                    lr=lr\n",
        "                )\n",
        "\n",
        "                trainer = Trainer(\n",
        "                    max_epochs=5,\n",
        "                    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                    devices=1,\n",
        "                    logger=CSVLogger(\"logs/\", name=f\"gridsearch_lr{lr}_do{do}\"),\n",
        "                    enable_progress_bar=True\n",
        "                )\n",
        "\n",
        "                trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "                val_acc = trainer.logged_metrics.get(\"val_acc\")\n",
        "                model_state = model.state_dict()\n",
        "                if val_acc and val_acc.item() > best_val_acc:\n",
        "                    best_val_acc = val_acc.item()\n",
        "                    best_params = {'learning_rate': lr, 'dropout': do}\n",
        "\n",
        "                model_path = f\"model_LR{lr}_DO{do}.pth\"\n",
        "                torch.save(model_state, model_path)\n",
        "                print(f\"Model saved at {model_path}\")\n",
        "\n",
        "if load_models:\n",
        "    for lr in param_grid['learning_rate']:\n",
        "        for do in param_grid['dropout']:\n",
        "            model = load_model_from_github(SNLIMLP, f'https://raw.github.com/FRAMAX444/Textual-Entailment/main/Hyperparamter_Tuning_1st_Model/model_LR{lr}_DO{do}.pth')\n",
        "            trainer = Trainer(\n",
        "                max_epochs=5,\n",
        "                accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                devices=1\n",
        "            )\n",
        "            results = trainer.test(model, val_loader)\n",
        "\n",
        "\n",
        "\n",
        "            val_acc = results[0]['test_acc']\n",
        "\n",
        "            if val_acc and val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {'learning_rate': lr, 'dropout': do}\n",
        "\n",
        "# Print the Best Hyperparameters\n",
        "print(\"\\nBest Hyperparameters:\", best_params)\n",
        "print(\"Best Validation Accuracy:\", best_val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E334KDVTVT17"
      },
      "source": [
        "Thus, the best model in terms of **accuracy** is the one with the following hyperparameters:\n",
        "\n",
        "```{'learning_rate': 0.0001, 'dropout': 0.1}```\n",
        "\n",
        "It achieved an **accuracy** of ```0.7448689341545105``` on the **Validation Set**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zODmUhIPWgvt"
      },
      "source": [
        "## 2.5 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc-lYkOJJhoL"
      },
      "source": [
        "We now proceed with training the **best model** for **10 epochs** with **Early Stopping** to prevent overfitting and then evaluate its performance on the **Test Set**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54lkGqvKWjxl"
      },
      "outputs": [],
      "source": [
        "EPOCHS=10\n",
        "torch.manual_seed(0)\n",
        "model = SNLIMLP(input_dim=10007, dropout = 0.1 , lr = 0.0001)\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"checkpoints/\",\n",
        "    save_top_k=2,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\"\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    min_delta=0.00,\n",
        "    patience=3,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=EPOCHS,\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    callbacks=[early_stopping, checkpoint_callback],\n",
        "    log_every_n_steps=1,\n",
        "    deterministic=True\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "trainer.test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O1CKpKuoCcN"
      },
      "outputs": [],
      "source": [
        "def plot_training_metrics(model):\n",
        "    if not model.training_losses or not model.validation_accuracies:\n",
        "        print(\"Not enough data to plot. Ensure that at least one epoch has been completed.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    loss_training = model.training_losses\n",
        "    loss_validation = model.validation_losses\n",
        "    accuracy = model.validation_accuracies\n",
        "\n",
        "    epochs = range(0, len(loss_validation))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs[1:],loss_training, marker='o', label=\"Training Loss\",color='blue')\n",
        "    plt.plot(epochs,loss_validation, marker='o', label=\"Validation Loss\", color='orange')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss Over Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, marker='o', label=\"Validation Accuracy\", color='orange')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Validation Accuracy Over Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_training_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUOlw76thVuP"
      },
      "source": [
        "Epoch 0 represents the validation loss and accuracy before the first training epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2a1Sc1DKJtD"
      },
      "source": [
        "---\n",
        "As we can see, the training loss decreases, while the validation loss initially drops but starts to rise again after the third epoch. This indicates that after this point, the model begins to overfit. Similarly, the right-hand graph shows that accuracy increases rapidly before epoch 3 and then stabilizes.\n",
        "\n",
        "To address this, we applied Early Stopping, to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOdImM_xoF3r"
      },
      "source": [
        "## 2.6 Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaEFjCFAilQ0"
      },
      "source": [
        "Now, we evaluate the model using the following metrics:\n",
        "\n",
        "1.   `Accuracy`\n",
        "2.   `Precision`\n",
        "3.   `Recall`\n",
        "4.   `F1-score`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SJNwVukMRRN"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for premise_tokens, hypothesis_tokens, labels in test_loader:\n",
        "\n",
        "            premise_tokens, hypothesis_tokens, labels = (\n",
        "                premise_tokens.to(device),\n",
        "                hypothesis_tokens.to(device),\n",
        "                labels.to(device),\n",
        "            )\n",
        "\n",
        "\n",
        "            logits = model(premise_tokens, hypothesis_tokens)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average=\"macro\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    # Results\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=[\"Contradiction\", \"Neutral\", \"Entailment\"],\n",
        "                yticklabels=[\"Contradiction\", \"Neutral\", \"Entailment\"])\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix for SNLI Model\")\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, precision, recall, f1, conf_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUxeuchdMUFW"
      },
      "outputs": [],
      "source": [
        "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiGwEKEUzaOc"
      },
      "source": [
        "# **3. Improved Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES9F76APJhoL"
      },
      "source": [
        "The second implemented model, explained in more detail in the report, is based on six key steps:\n",
        "\n",
        "1.   **Tokenization**\n",
        "2.   **Dataloader**\n",
        "3.   **Model Definition**\n",
        "4.   **Hyperparameter Tuning**\n",
        "5.   **Training**\n",
        "6.   **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBx1QJpFBXU"
      },
      "source": [
        "## 3.1 Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFh5BF8zjiT6"
      },
      "source": [
        "We transform each sentence into a vector of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2X7Qk_v2w2M"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    cleaned_sentence = \"\".join(char if char.isalnum() or char.isspace() else \"\" for char in sentence)\n",
        "    return cleaned_sentence.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOECQAIq2ymB"
      },
      "outputs": [],
      "source": [
        "tokenize('Ciao, mi chi^=)(&$\"\"/£)\"(amo Francesco e sono uno studente!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miQGtthIktEG"
      },
      "source": [
        "We create a vocabulary where:\n",
        "\n",
        "1. Each word in the **training set** is assigned a unique number.\n",
        "2. All **unknown words** appearing in the **test/validation set** are mapped to **1**.\n",
        "3. Each sentence is represented as a **vector of numbers**, where each number corresponds to a word. The maximum length is **20 words**, as **95% of the sentences in the dataset** contain **fewer than 20 words**.\n",
        "4. If a sentence has **more than 20 words**, it is truncated to **20**.\n",
        "5. If a sentence has **fewer than 20 words**, it is **padded with 0s**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol8CA3hq204M"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "        self.idx2word = [\"<PAD>\", \"<UNK>\"]\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in tokenize(sentence):\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = len(self.idx2word)\n",
        "                self.idx2word.append(word)\n",
        "\n",
        "    def encode(self, sentence, max_len=20):\n",
        "        tokens = [self.word2idx.get(word, 1) for word in tokenize(sentence)]\n",
        "        return tokens[:max_len] + [0] * (max_len - len(tokens))\n",
        "\n",
        "    def __str__(self):\n",
        "        vocab_size = len(self.idx2word)\n",
        "\n",
        "        preview_size = min(10, vocab_size)\n",
        "        vocab_preview = \", \".join(self.idx2word[:preview_size])\n",
        "        idx_preview = \", \".join(str(self.word2idx[word]) for word in self.idx2word[:preview_size])\n",
        "\n",
        "        return (f\"Vocabulary(size={vocab_size}):\\n\"\n",
        "                f\"Words: [{vocab_preview}, ...]\\n\"\n",
        "                f\"Indices: [{idx_preview}, ...]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHIAzP1HqbuL"
      },
      "source": [
        "We build the vocabulary and print it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g5LK2PT23XI"
      },
      "outputs": [],
      "source": [
        "vocab = Vocabulary()\n",
        "\n",
        "for _, row in train_df.iterrows():\n",
        "    vocab.add_sentence(row['sentence1'])\n",
        "    vocab.add_sentence(row['sentence2'])\n",
        "\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhOxFxSrqgR4"
      },
      "source": [
        "We visualize how a sentence is encoded into the vector described above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl7LDLkc256l"
      },
      "outputs": [],
      "source": [
        "i=37521\n",
        "test_sentence = train_df.loc[i, 'sentence1']\n",
        "\n",
        "encoded_sentence = vocab.encode(test_sentence)\n",
        "\n",
        "print(f\"Encoded '{test_sentence}': {encoded_sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCRPc4EKFIX9"
      },
      "source": [
        "## 3.2 DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQPte121q2wq"
      },
      "source": [
        "We implement our dataset using the `class SNLIDataset(Dataset)`, where the data consists of:\n",
        "\n",
        "1. **Input**: Two vectors corresponding to the two sentences (premise and hypothesis).\n",
        "2. **Output**: The `gold_label`.\n",
        "\n",
        "Additionally, we include the **vocabulary** and set the **maximum sentence length to 20**, as described earlier.\n",
        "\n",
        "Next, we create three datasets corresponding to the **training, validation, and test sets** using the defined class. Finally, we implement the respective data loaders using `Dataloader` with a with a **batch size of 512** for training set and **64** for validation and test set.  .  \n",
        "\n",
        "Moreover, we shuffle (**shuffle=True**) only the **training set** to prevent the model from learning any specific order in the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV0k4W3j5iuW"
      },
      "outputs": [],
      "source": [
        "data_train=[\n",
        "    (row['sentence1'], row['sentence2'], row['gold_label'])\n",
        "    for _, row in train_df.iterrows()\n",
        "]\n",
        "\n",
        "data_val=[\n",
        "    (row['sentence1'], row['sentence2'], row['gold_label'])\n",
        "    for _, row in val_df.iterrows()\n",
        "]\n",
        "\n",
        "data_test=[\n",
        "    (row['sentence1'], row['sentence2'], row['gold_label'])\n",
        "    for _, row in test_df.iterrows()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmQa9NR53EdP"
      },
      "outputs": [],
      "source": [
        "class SNLIDataset(Dataset):\n",
        "    def __init__(self, data, vocab, max_len=20):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        premise, hypothesis, label = self.data[idx]\n",
        "        return (\n",
        "            torch.tensor(self.vocab.encode(premise, self.max_len), dtype=torch.long),\n",
        "            torch.tensor(self.vocab.encode(hypothesis, self.max_len), dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4erFd1R3Grv"
      },
      "outputs": [],
      "source": [
        "train_dataset = SNLIDataset(data_train, vocab)\n",
        "val_dataset = SNLIDataset(data_val, vocab)\n",
        "test_dataset = SNLIDataset(data_test, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGM1udKn3Mf_"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSaWLEqxFLch"
      },
      "source": [
        "## 3.3 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDTJML2tJhoM"
      },
      "source": [
        "### Neural Network Architecture\n",
        "\n",
        "For simplicity, when specifying tensor dimensions, we omit the **batch size** (e.g., if the batch size is 512, we write `(512, 20) -> (20)`).\n",
        "\n",
        "The model takes as **input** two vectors of size **20**, corresponding to the **premise** and the **hypothesis**.\n",
        "\n",
        "To better understand the architecture, we analyze **each input separately**, starting with the **premise**:\n",
        "\n",
        "#### **1. Embedding Layer**\n",
        "Each element (word) in the input vector is passed through an **Embedding layer**, which transforms it into a vector of size **300**.  \n",
        "Thus, we obtain a tensor of **size (20, 300)**, where:\n",
        "- `20` is the number of words in the sentence.\n",
        "- `300` is the embedding dimension for each word.\n",
        "\n",
        "#### **2. Bidirectional LSTM**\n",
        "The resulting tensor is passed through a **Bidirectional LSTM**, which processes the sequence **both forward and backward** to capture contextual meaning from previous and subsequent words.  \n",
        "The LSTM consists of **two layers**, with **dropout applied only after the first layer** (not on the final output).  \n",
        "\n",
        "The output tensor has size **(20, 400)**, where:\n",
        "- `20` is the sequence length.\n",
        "- `200 * 2 = 400` (since the LSTM is bidirectional, it concatenates forward and backward outputs).\n",
        "\n",
        "#### **3. Mean-Max Pooling**\n",
        "To reduce dimensionality while retaining the most important information, we apply **Mean-Max Pooling**, which performs:\n",
        "\n",
        "- **Mean Pooling**: Computes the average across the sequence dimension, producing a vector of size **400**.\n",
        "- **Max Pooling**: Computes the maximum value across the sequence dimension, also producing a vector of size **400**.\n",
        "\n",
        "We then **concatenate** the two outputs.  \n",
        "The final output after Mean-Max Pooling has size **400 * 2 = 800**.\n",
        "\n",
        "We repeat the **same process** for the **hypothesis**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Feature Combination**\n",
        "After processing both inputs, we obtain two vectors of **size 800** (one for the premise and one for the hypothesis). We then compute:\n",
        "\n",
        "1. **Premise representation** (size **800**).\n",
        "2. **Hypothesis representation** (size **800**).\n",
        "3. **Absolute difference** between the two vectors to highlight differences (**size 800**).\n",
        "4. **Element-wise product** between the two vectors to emphasize similarity (**size 800**).\n",
        "\n",
        "We **concatenate** these four vectors, resulting in a final **feature vector of size (800 + 800 + 800 + 800 = 3200)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Classification**\n",
        "The concatenated vector is passed through a **Multi-Layer Perceptron (MLP)**:\n",
        "1. First, the **MLP** reduces the vector to size **300**, applying:\n",
        "   - **ReLU activation**\n",
        "   - **Dropout**\n",
        "2. Finally, a fully connected **output layer** maps the **300-dimensional vector** to the **three target classes**:  \n",
        "   `{Entailment, Contradiction, Neutral}`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztR4N65R9sEb"
      },
      "outputs": [],
      "source": [
        "class SNLI_LSTM(pl.LightningModule):\n",
        "    def __init__(self, hidden_dim=200, vocab_size = len(vocab.idx2word), embedding_dim=300, output_dim=3,\n",
        "                 num_layers=2, dropout=0.3, learning_rate=1e-3):\n",
        "        super(SNLI_LSTM, self).__init__()\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.training_losses = []\n",
        "        self.validation_accuracies = []\n",
        "        self.validation_losses = []\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 16, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(300, output_dim)\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=output_dim)\n",
        "\n",
        "    def mean_max_pooling(self, lstm_output):\n",
        "        mean_pool = torch.mean(lstm_output, dim=1)\n",
        "        max_pool, _ = torch.max(lstm_output, dim=1)\n",
        "        return torch.cat((mean_pool, max_pool), dim=1)\n",
        "\n",
        "    def forward(self, premise, hypothesis):\n",
        "        premise_embed = self.embedding(premise)\n",
        "        hypothesis_embed = self.embedding(hypothesis)\n",
        "\n",
        "        premise_output, _ = self.lstm(premise_embed)\n",
        "        hypothesis_output, _ = self.lstm(hypothesis_embed)\n",
        "\n",
        "        premise_vector = self.mean_max_pooling(premise_output)\n",
        "        hypothesis_vector = self.mean_max_pooling(hypothesis_output)\n",
        "\n",
        "        combined = torch.cat((premise_vector, hypothesis_vector,\n",
        "                              torch.abs(premise_vector - hypothesis_vector),\n",
        "                              premise_vector * hypothesis_vector), dim=1)\n",
        "\n",
        "        output = self.fc(combined)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        premise, hypothesis, labels = batch\n",
        "        logits = self(premise, hypothesis)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        acc = self.accuracy(logits, labels)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        premise, hypothesis, labels = batch\n",
        "        logits = self(premise, hypothesis)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        acc = self.accuracy(logits, labels)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        try:\n",
        "            if self.trainer.logged_metrics[\"train_loss\"].cpu().item() not in self.training_losses:\n",
        "                self.training_losses.append(self.trainer.logged_metrics[\"train_loss\"].cpu().item())\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        premise, hypothesis, labels = batch\n",
        "        logits = self(premise, hypothesis)\n",
        "        acc = self.accuracy(logits, labels)\n",
        "        self.log(\"test_acc\", acc, prog_bar=True)\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_train_loss = self.trainer.logged_metrics[\"train_loss\"].cpu().item()\n",
        "        print(f\"[Train] Epoch {self.current_epoch+1} - Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        avg_val_loss = self.trainer.logged_metrics[\"val_loss\"].cpu().item()\n",
        "        avg_val_acc = self.trainer.logged_metrics[\"val_acc\"].cpu().item()\n",
        "        try:\n",
        "            if self.trainer.logged_metrics[\"val_acc\"].cpu().item() not in self.validation_accuracies:\n",
        "                self.validation_accuracies.append(self.trainer.logged_metrics[\"val_acc\"].cpu().item())\n",
        "            if self.trainer.logged_metrics[\"val_loss\"].cpu().item() not in self.validation_losses:\n",
        "                self.validation_losses.append(self.trainer.logged_metrics[\"val_loss\"].cpu().item())\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(f\"[Validation] Epoch {self.current_epoch+1} - Loss: {avg_val_loss:.4f}, Accuracy: {avg_val_acc:.4f}\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), self.learning_rate, weight_decay=1e-3)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=2, factor=0.5)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HT8DEcmDtMeq"
      },
      "outputs": [],
      "source": [
        "#@title Model Visualization\n",
        "from graphviz import Digraph\n",
        "from IPython.display import display\n",
        "\n",
        "def draw_snli_lstm_classifier():\n",
        "    dot = Digraph(format=\"png\")\n",
        "    dot.attr(rankdir='LR')  # Disposizione orizzontale\n",
        "\n",
        "    # Input Layers\n",
        "    dot.node(\"P\", \"Premise Input\\n(input_dim = 20)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightblue\")\n",
        "    dot.node(\"H\", \"Hypothesis Input\\n(input_dim = 20)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightblue\")\n",
        "\n",
        "    # Embedding Layer\n",
        "    dot.node(\"Emb_P\", \"Embedding Layer\\n(20,embedding_size = 300)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightgreen\")\n",
        "    dot.node(\"Emb_H\", \"Embedding Layer\\n(20,embedding_size = 300)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightgreen\")\n",
        "\n",
        "\n",
        "    # BiLSTM Layer\n",
        "    dot.node(\"LSTM_P\", \"BiLSTM Layer\\n(20,200*2)\\n2 Layer\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightyellow\")\n",
        "    dot.node(\"LSTM_H\", \"BiLSTM Layer\\n(20,200*2)\\n2 Layer\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightyellow\")\n",
        "\n",
        "    # Pooling\n",
        "    dot.node(\"Pool_P\", \"Mean-Max Pooling (2*400)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"orange\")\n",
        "    dot.node(\"Pool_H\", \"Mean-Max Pooling (2*400)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"orange\")\n",
        "\n",
        "\n",
        "\n",
        "    # Feature Combination\n",
        "    dot.node(\"Concat\", \"Feature Concatenation\\n(800+800+800+800)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightgrey\")\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    dot.node(\"FC1\", \"Linear (3200)\\nReLU\\nDropout\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightpink\")\n",
        "    dot.node(\"FC2\", \"Linear (300)\\n\", shape=\"rectangle\", style=\"filled\", fillcolor=\"lightcoral\")\n",
        "\n",
        "    # Output Classes\n",
        "    dot.node(\"Class1\", \"Entailment (class 0)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"red\")\n",
        "    dot.node(\"Class2\", \"Contradiction (class 1)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"red\")\n",
        "    dot.node(\"Class3\", \"Neutral (class 2)\", shape=\"rectangle\", style=\"filled\", fillcolor=\"red\")\n",
        "\n",
        "    # Edges\n",
        "    dot.edge(\"P\", \"Emb_P\")\n",
        "    dot.edge(\"H\", \"Emb_H\")\n",
        "    dot.edge(\"Emb_P\", \"LSTM_P\")\n",
        "    dot.edge(\"Emb_H\", \"LSTM_H\")\n",
        "    dot.edge(\"LSTM_P\", \"Pool_P\")\n",
        "    dot.edge(\"LSTM_H\", \"Pool_H\")\n",
        "    dot.edge(\"Pool_P\", \"Concat\")\n",
        "    dot.edge(\"Pool_H\", \"Concat\")\n",
        "    dot.edge(\"Concat\", \"FC1\")\n",
        "    dot.edge(\"FC1\", \"FC2\")\n",
        "    dot.edge(\"FC2\", \"Class1\")\n",
        "    dot.edge(\"FC2\", \"Class2\")\n",
        "    dot.edge(\"FC2\", \"Class3\")\n",
        "\n",
        "    display(dot)\n",
        "\n",
        "draw_snli_lstm_classifier()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOlm8Q14YJ2m"
      },
      "source": [
        "## 3.4 Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLByTuFeJhoN"
      },
      "source": [
        "If you want to **retrain the models**, set `load_models=False`.  \n",
        "\n",
        "Otherwise, if `load_models=True`, the pre-trained models will be loaded using the `load_model_from_github` function defined above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PaB5XZeciZR"
      },
      "outputs": [],
      "source": [
        "# Track best results\n",
        "best_val_acc = 0\n",
        "best_params = {}\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_dim': [200, 300],\n",
        "    'learning_rate': [0.001, 0.0001],\n",
        "    'dropout': [0.1, 0.3]\n",
        "}\n",
        "\n",
        "\n",
        "if not load_models:\n",
        "    for h_dim in param_grid['hidden_dim']:\n",
        "        for lr in param_grid['learning_rate']:\n",
        "            for do in param_grid['dropout']:\n",
        "                print(f\"\\nTraining with hidden_dim={h_dim}, learning_rate={lr}, dropout={do}\")\n",
        "\n",
        "\n",
        "                model = SNLI_LSTM(\n",
        "                    hidden_dim=h_dim,\n",
        "                    vocab_size=len(vocab.idx2word),\n",
        "                    dropout=do,\n",
        "                    learning_rate=lr\n",
        "                )\n",
        "\n",
        "\n",
        "                trainer = Trainer(\n",
        "                    max_epochs=5,\n",
        "                    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                    devices=1,\n",
        "                    logger=CSVLogger(\"logs/\", name=f\"gridsearch_h{h_dim}_lr{lr}_do{do}\"),\n",
        "                    enable_progress_bar=True,\n",
        "                )\n",
        "\n",
        "\n",
        "                trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "                # Save Model after training\n",
        "                model_path = f\"model_HD{h_dim}_LR{lr}_DO{do}.pth\"\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print(f\"Model saved at {model_path}\")\n",
        "\n",
        "                # Define the path where you want to save the model on Google Drive\n",
        "                drive_model_path = '/content/drive/MyDrive/model_HD{}_LR{}_DO{}.pth'\n",
        "\n",
        "                # Save model to Google Drive\n",
        "                drive_path = drive_model_path.format(h_dim, lr, do)\n",
        "                torch.save(model.state_dict(), drive_path)\n",
        "                print(f\"Model saved to Google Drive at {drive_path}\")\n",
        "\n",
        "if load_models:\n",
        "    for h_dim in param_grid['hidden_dim']:\n",
        "        for lr in param_grid['learning_rate']:\n",
        "            for do in param_grid['dropout']:\n",
        "                model = load_model_from_github(SNLI_LSTM, f'https://raw.github.com/FRAMAX444/Textual-Entailment/main/Hyperparamter_Tuning_2nd_Model/model_HD{h_dim}_LR{lr}_DO{do}.pth', h_dim, False)\n",
        "                trainer = Trainer(\n",
        "                    max_epochs=5,\n",
        "                    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                    devices=1\n",
        "                )\n",
        "                results = trainer.test(model, val_loader)\n",
        "\n",
        "                # Extracting the test accuracy\n",
        "                val_acc = results[0]['test_acc']  # Extract test accuracy\n",
        "                print(f\"Test Accuracy for hidden_dim={h_dim}, learning_rate={lr}, dropout={do}: {val_acc}\")\n",
        "\n",
        "                if val_acc and val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    best_params = {'hidden_dimension':h_dim, 'learning_rate': lr, 'dropout': do}\n",
        "\n",
        "# Print the Best Hyperparameters\n",
        "print(\"\\nBest Hyperparameters:\", best_params)\n",
        "print(\"Best Validation Accuracy:\", best_val_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLCYzlMcYitg"
      },
      "source": [
        "Thus, the best model is the one with the following hyperparameters:\n",
        "\n",
        "```{'hidden_dim': 300, 'learning_rate': 0.0001, 'dropout': 0.1}```\n",
        "\n",
        "\n",
        "It achieved an **accuracy** of ```0.8204632997512817``` on the **Validation Set**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AydDbNiVZUVt"
      },
      "source": [
        "## 3.5 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ndyVAsJhoN"
      },
      "source": [
        "We now proceed with **training the best model for 10 epochs** and then evaluate its performance on the **Test Set**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdSBiN5L-xcI"
      },
      "outputs": [],
      "source": [
        "EPOCHS=10\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = SNLI_LSTM(hidden_dim=300, vocab_size = len(vocab.idx2word), dropout=0.1, learning_rate=0.001)\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=\"checkpoints/\",\n",
        "    save_top_k=2,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\"\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    min_delta=0.00,\n",
        "    patience=3,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=EPOCHS,\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    callbacks=[early_stopping, checkpoint_callback],\n",
        "    log_every_n_steps=1,\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "trainer.test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tEPILo9sKJz"
      },
      "outputs": [],
      "source": [
        "plot_training_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PExDbC_njot6"
      },
      "source": [
        "Epoch 0 represents the validation loss and accuracy before the first training epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwVJmpQ5jXlI"
      },
      "source": [
        "---\n",
        "As we can see, the training loss decreases, while the validation loss initially drops but starts to rise again after the third epoch. This indicates that after this point, the model begins to overfit. Similarly, the right-hand graph shows that accuracy increases rapidly before epoch 3 and then stabilizes.\n",
        "\n",
        "To address this, we applied Early Stopping, to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdZN7iNPZYI_"
      },
      "source": [
        "## 3.6 Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWPK-44jJhoN"
      },
      "source": [
        "Now, we evaluate the model using the following metrics:\n",
        "\n",
        "1. **Accuracy**\n",
        "2. **Precision**\n",
        "3. **Recall**\n",
        "4. **F1-score**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgjM2qPKGKkT"
      },
      "outputs": [],
      "source": [
        "accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BoLREs_1XKN"
      },
      "source": [
        "# **Conclusions and final report**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16qJcCFRJhoN"
      },
      "source": [
        "## **Objective**\n",
        "\n",
        "The goal of this project is to determine the relationship between two sentences, referred to as **premise** and **hypothesis**, and classify them into three categories:\n",
        "\n",
        "- **Entailment**: The second sentence logically follows from the first.\n",
        "- **Contradiction**: The second sentence contradicts the first.\n",
        "- **Neutral**: There is no clear relationship between the two sentences.\n",
        "\n",
        "To achieve this, we implemented and compared two machine learning models:\n",
        "\n",
        "- **Baseline Model**: A Multi-Layer Perceptron (MLP) that takes as input sentence vectors generated using **Hashing Vectorization**.\n",
        "- **Improved Model**: A BiLSTM + MLP model that processes sentences at the **word level**, where each word is passed through an **embedding layer**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Data Preprocessing**\n",
        "\n",
        "The dataset consists of sentence pairs labeled as **entailment, contradiction, or neutral**. The dataset was split as follows:\n",
        "\n",
        "- **Training set**: 550,152 samples\n",
        "- **Validation set**: 10,000 samples\n",
        "- **Test set**: 10,000 samples\n",
        "\n",
        "Each row contains the **premise, hypothesis**, and **target class**, labeled as *sentence1*, *sentence2*, and *gold_label*, respectively.\n",
        "\n",
        "### **Filtering Invalid Labels**\n",
        "We first removed any invalid labels (i.e., those not belonging to the three predefined categories), ensuring **data consistency**. The updated dataset sizes were:\n",
        "\n",
        "- **Training set**: 549,367 samples\n",
        "- **Validation set**: 9,842 samples\n",
        "- **Test set**: 9,842 samples\n",
        "\n",
        "### **Mapping Labels to Numerical Values**\n",
        "To facilitate model training, we mapped each label to a numerical value:\n",
        "\n",
        "- *entailment* → `0`\n",
        "- *contradiction* → `1`\n",
        "- *neutral* → `2`\n",
        "\n",
        "### **Sentence Length Analysis**\n",
        "We analyzed sentence lengths and observed that:\n",
        "\n",
        "- The **95th percentile** of sentence lengths was **23 words** for premises and **13 words** for hypotheses.\n",
        "- When considering **both premises and hypotheses**, the **95th percentile** of sentence lengths was **20 words**.\n",
        "\n",
        "This analysis helped determine the **maximum sequence length** for tokenization in the improved model.\n",
        "\n",
        "### **Dataset Balance**\n",
        "A **pie chart** visualization showed that the dataset distribution was **balanced**, ensuring that the model would not be biased toward any particular class.\n",
        "\n",
        "---\n",
        "\n",
        "## **Baseline Model**\n",
        "\n",
        "### **Data Handling**\n",
        "We used **Hashing Vectorization** to convert both sentences into **10,007-dimensional sparse vectors**, ensuring a consistent input size for the model.\n",
        "\n",
        "### **Model Architecture**\n",
        "The model consists of two main networks:\n",
        "\n",
        "#### **1. First Network (`first_net`)**\n",
        "This network processes the **premise and hypothesis separately**, reducing the dimensionality of the input representations:\n",
        "\n",
        "- **Linear Layers**: Project the **original 10,007-dimensional** feature space to **1,024**, then **512 dimensions**.\n",
        "- **Batch Normalization**: Improves training stability.\n",
        "- **LeakyReLU Activation**: Introduces non-linearity.\n",
        "- **Dropout**: Prevents overfitting.\n",
        "\n",
        "#### **2. Combined Network (`combined_net`)**\n",
        "This network combines the **premise and hypothesis representations** (each of size **512**) to produce the final classification:\n",
        "\n",
        "- **Linear Layers**: Transform the concatenated representation (512 × 2 = **1,024**) to **256**, then **128 dimensions**.\n",
        "- **Batch Normalization**: Stabilizes training.\n",
        "- **LeakyReLU Activation**: Introduces non-linearity.\n",
        "- **Dropout**: Reduces overfitting.\n",
        "- **Final Linear Layer**: Maps the **128-dimensional** representation to a **3-class output**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Results**\n",
        "\n",
        "After training the model, the **results** with all the metrics (precision, accuracy, recall and F1-score) are very high all around 0.75 as shown in section 2.6.\n",
        "\n",
        "---\n",
        "\n",
        "## **Improved Model**\n",
        "\n",
        "To improve performance, we implemented a **Bidirectional LSTM (BiLSTM)** with **word embeddings**, as it captures **sequential relationships between words**, unlike the hashing vectorized model.\n",
        "\n",
        "### **Data Handling**\n",
        "We performed the following preprocessing steps for each sentence:\n",
        "\n",
        "1. **Lowercasing text**\n",
        "2. **Removing special characters**\n",
        "3. **Splitting sentences into words**\n",
        "4. **Building a vocabulary dictionary**, mapping words to numerical values\n",
        "5. **Padding/truncating** each sentence to the **maximum sequence length (20 words)**\n",
        "\n",
        "---\n",
        "\n",
        "## **Architecture**\n",
        "\n",
        "For simplicity, we omit the batch size in tensor shapes (e.g., if batch size is 64, `(64, 20) → (20)`).\n",
        "\n",
        "The model takes as input **two vectors of size 20** (one for the **premise** and one for the **hypothesis**).\n",
        "\n",
        "We analyze each input separately, starting with the **premise**:\n",
        "\n",
        "### **1. Embedding Layer**\n",
        "Each word is passed through an **embedding layer**, converting it into a **300-dimensional vector**.  \n",
        "The output tensor has shape **(20, 300)**, where:\n",
        "- `20` = sentence length\n",
        "- `300` = embedding dimension\n",
        "\n",
        "### **2. Bidirectional LSTM**\n",
        "The output is passed to a **BiLSTM**, which reads the sequence **both forward and backward**.  \n",
        "The final output has shape **(20, 400)**, where:\n",
        "- `200 × 2 = 400` (since the LSTM is bidirectional).\n",
        "\n",
        "### **3. Mean-Max Pooling**\n",
        "To extract the most relevant information, we apply **Mean-Max Pooling**, which computes:\n",
        "- **Mean Pooling** → size **400**\n",
        "- **Max Pooling** → size **400**\n",
        "\n",
        "The two outputs are concatenated, resulting in a **final size of 800**.\n",
        "\n",
        "The same process is applied to the **hypothesis**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Feature Combination**\n",
        "We obtain two feature vectors of size **800** each. We then compute:\n",
        "\n",
        "1. **Premise representation** → size **800**\n",
        "2. **Hypothesis representation** → size **800**\n",
        "3. **Absolute difference** between the two vectors (**800**)\n",
        "4. **Element-wise product** between the two vectors (**800**)\n",
        "\n",
        "After concatenation, the final vector has size **3,200**, which is passed through:\n",
        "\n",
        "- **MLP** (with ReLU activation & Dropout) → reduces to **300 dimensions**.\n",
        "- **Final Linear Layer** → maps **300** to the **3-class output** `{Entailment, Contradiction, Neutral}`.\n",
        "\n",
        "---\n",
        "\n",
        "## **Results**\n",
        "\n",
        "After training the model, the **results** with all the metrics (precision, accuracy, recall and F1-score) are very high all around 0.81 as shown in section 3.6.\n",
        "\n",
        "---\n",
        "\n",
        "## **Model Comparison**\n",
        "\n",
        "The **BiLSTM model** outperformed the baseline for four main reasons:\n",
        "\n",
        "1. **Contextual Understanding**  \n",
        "   BiLSTMs process text **bidirectionally**, allowing them to capture word meaning based on **preceding and following words**.\n",
        "\n",
        "2. **Sequential Dependencies**  \n",
        "   Unlike the **hashing vectorized MLP**, which treats sentences as **bags of words**, the BiLSTM **preserves word order**, improving sentence representation.\n",
        "\n",
        "3. **Improved Sentence Representations**  \n",
        "   - Embeddings + BiLSTM produce **dense, meaningful representations**.\n",
        "   - Mean-Max Pooling extracts the **most relevant** sentence features.\n",
        "   - The **hashing vectorizer** used in the baseline **introduces noise** due to hash collisions.\n",
        "\n",
        "4. **Enhanced Relationship Modeling**  \n",
        "   The BiLSTM+MLP model explicitly calculates:\n",
        "   - **Absolute difference** between sentence representations (highlighting differences).\n",
        "   - **Element-wise product** (capturing similarity).\n",
        "   The baseline model **lacks this information**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Limitations**\n",
        "- **Long Sentences**: BiLSTMs struggle with very long sequences.\n",
        "- **Ambiguous Sentences**: The model may misclassify ambiguous cases.\n",
        "- **Out-of-Distribution Data**: Performance drops on unseen sentence structures.\n",
        "- **Colab GPU Constraints**: Limited hyperparameter tuning due to compute restrictions.\n",
        "\n",
        "---\n",
        "## **Potential Improvements**\n",
        "\n",
        "In the processes described above, improvements can be made at different levels:\n",
        "\n",
        "### **1. Data Handling** ###  \n",
        "\n",
        "- **Word Frequency Consideration**: Rare words might provide more information than frequent ones. For example, we could try to eliminate words that appear too often, as they are likely articles or words with little meaningful contribution.\n",
        "\n",
        "### **2. Hyperparameter Tuning** ###\n",
        "\n",
        "- **Bayesian Optimization Approach**: The models described above use Grid Search, but due to Colab's limited runtime and the long training times, the hyperparameter set is restricted. For this reason, other approaches such as Bayesian optimization could be used to maximize the search for optimal hyperparameters.\n",
        "\n",
        "- **Expand the Hyperparameter Search Space**: A more exhaustive grid search over a broader range of parameters, including learning rates, dropout rates, and weight decay values, could lead to superior model configurations.\n",
        "\n",
        "### **3. Model** ###  \n",
        "\n",
        "- **Transformers**: Transformer models are much more effective at capturing context because they do not read sequentially like LSTMs but analyze all words simultaneously. This prevents information loss in long sentences and reduces vanishing gradient issues.\n",
        "\n",
        "- **Attention Mechanism**: An LSTM with an attention mechanism can improve performance by better capturing the most important words in a sentence. Instead of treating each word equally, the model can focus on key words.\n",
        "\n",
        "- **Use of Pre-Trained Word Embeddings (e.g., GloVe, FastText, Word2Vec)**  \n",
        "  Instead of randomly initializing word embeddings, we can use pre-trained vectors. This helps the model understand word meanings better and faster.\n",
        "\n",
        "- **Word Frequency**: We could consider incorporating word frequency not only in data handling but also in the model, giving more importance to less frequently occurring words.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uTVF86Tagi35",
        "cfQudzomg33X"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
